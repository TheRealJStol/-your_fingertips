{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4e414b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading zeroâ€‘shot model â€¦ "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jared\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jared\\.cache\\huggingface\\hub\\models--valhalla--distilbart-mnli-12-3. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "import torch                              # import *first*\n",
    "from transformers import pipeline         # transformers sees torch now\n",
    "import requests, networkx as nx\n",
    "import os, time, itertools, pickle, functools, re, sys\n",
    "\n",
    "print(\"loading zeroâ€‘shot model â€¦ \", end=\"\", flush=True)\n",
    "zero_shot = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"valhalla/distilbart-mnli-12-3\",\n",
    ")\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9454bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€ free local LLM gatekeeper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "CANDIDATES = [\"mathematics\", \"not mathematics\"]\n",
    "\n",
    "@functools.lru_cache(maxsize=8192)\n",
    "def is_math_topic(label: str) -> bool:\n",
    "    \"\"\"Cheap heuristic first, else free MNLI classifier.\"\"\"\n",
    "    label_low = label.lower()\n",
    "    if re.search(r\"\\b(algebra|geometry|calculus|number theory|mathem)\", label_low):\n",
    "        return True\n",
    "    if re.search(r\"\\b(music|film|politic|football|chemical|history)\", label_low):\n",
    "        return False\n",
    "    out = zero_shot(label, CANDIDATES)\n",
    "    score = dict(zip(out[\"labels\"], out[\"scores\"]))[\"mathematics\"]\n",
    "    return score >= 0.5           # tweak threshold if desired\n",
    "\n",
    "# â”€â”€â”€â”€â”€ Wikidata crawl parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SPARQL   = \"https://query.wikidata.org/sparql\"\n",
    "HEADERS  = {\"User-Agent\": \"MathGraph-FreeLLM/1.0\"}    # ASCII only\n",
    "ROOTS    = [\"Q395\"]           # Mathematics\n",
    "CHUNK    = 100               # Qâ€‘ids per VALUES clause\n",
    "PAUSE    = 0.1               # polite delay (s) between WDQS calls\n",
    "MAX_NODES = 8000             # safety cap so you donâ€™t DOS yourself\n",
    "\n",
    "# â”€â”€â”€â”€â”€ helper functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def sparql(q: str):\n",
    "    \"\"\"POST query, return row list, die noisily on error.\"\"\"\n",
    "    r = requests.post(SPARQL,\n",
    "                      data={\"query\": q, \"format\": \"json\"},\n",
    "                      headers=HEADERS, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"results\"][\"bindings\"]\n",
    "\n",
    "def batches(seq, n):\n",
    "    it = iter(seq)\n",
    "    while (chunk := list(itertools.islice(it, n))):\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5289a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â€£ crawling Wikidata â€¦\n",
      "  depth 1: +51 nodes (total 52)\n",
      "  depth 2: +98 nodes (total 150)\n",
      "  depth 3: +52 nodes (total 202)\n",
      "  depth 4: +9 nodes (total 211)\n",
      "  depth 5: +3 nodes (total 214)\n",
      "  depth 6: +0 nodes (total 214)\n",
      "âœ” StageÂ 1 done â†’ 214 nodes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€ StageÂ 1 â€“ breadthâ€‘first crawl with LLM filter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "all_nodes   = set(ROOTS)\n",
    "frontier    = set(ROOTS)\n",
    "depth       = 0\n",
    "\n",
    "print(\"\\nâ€£ crawling Wikidata â€¦\")\n",
    "while frontier and len(all_nodes) < MAX_NODES:\n",
    "    depth += 1\n",
    "    next_frontier = set()\n",
    "\n",
    "    for chunk in batches(frontier, CHUNK):\n",
    "        vals = \" \".join(f\"wd:{q}\" for q in chunk)\n",
    "        q = f\"\"\"\n",
    "        SELECT DISTINCT ?child ?childLabel WHERE {{\n",
    "          VALUES ?parent {{ {vals} }}\n",
    "          ?child (wdt:P279|wdt:P31) ?parent .\n",
    "          SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        for row in sparql(q):\n",
    "            child = row[\"child\"][\"value\"].rsplit(\"/\", 1)[-1]\n",
    "            label = row[\"childLabel\"][\"value\"]\n",
    "            if child not in all_nodes and is_math_topic(label):\n",
    "                all_nodes.add(child)\n",
    "                next_frontier.add(child)\n",
    "        time.sleep(PAUSE)\n",
    "\n",
    "    print(f\"  depth {depth}: +{len(next_frontier):,} nodes \"\n",
    "          f\"(total {len(all_nodes):,})\")\n",
    "    frontier = next_frontier\n",
    "\n",
    "print(f\"âœ” StageÂ 1 done â†’ {len(all_nodes):,} nodes\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b0f378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€£ fetching intraâ€‘set edges â€¦\n",
      "âœ” StageÂ 2 done â†’ 259 edges\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€ StageÂ 2 â€“ collect edges within node set â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "edges = []\n",
    "node_set = set(all_nodes)\n",
    "print(\"â€£ fetching intraâ€‘set edges â€¦\")\n",
    "\n",
    "for chunk in batches(all_nodes, CHUNK):\n",
    "    vals = \" \".join(f\"wd:{q}\" for q in chunk)\n",
    "    q = f\"\"\"\n",
    "    SELECT ?parent ?child WHERE {{\n",
    "      VALUES ?child {{ {vals} }}\n",
    "      {{ ?child (wdt:P279|wdt:P31|wdt:P361) ?parent. }}\n",
    "      UNION\n",
    "      {{ ?parent wdt:P361 ?child. }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    for row in sparql(q):\n",
    "        p = row[\"parent\"][\"value\"].rsplit(\"/\", 1)[-1]\n",
    "        c = row[\"child\"][\"value\"].rsplit(\"/\", 1)[-1]\n",
    "        if p in node_set and c in node_set:\n",
    "            edges.append((p, c))\n",
    "    time.sleep(PAUSE)\n",
    "\n",
    "edges = list(set(edges))\n",
    "print(f\"âœ” StageÂ 2 done â†’ {len(edges):,} edges\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "952a4c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€£ downloading labels â€¦\n",
      "âœ” labels fetched (188)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€ StageÂ 3 â€“ English labels for remaining nodes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "labels = {}\n",
    "print(\"â€£ downloading labels â€¦\")\n",
    "for chunk in batches(all_nodes, 200):\n",
    "    vals = \" \".join(f\"wd:{q}\" for q in chunk)\n",
    "    q = f\"\"\"\n",
    "    SELECT ?id ?label WHERE {{\n",
    "      VALUES ?id {{ {vals} }}\n",
    "      ?id rdfs:label ?label .\n",
    "      FILTER (lang(?label) = \"en\")\n",
    "    }}\n",
    "    \"\"\"\n",
    "    for row in sparql(q):\n",
    "        qid   = row[\"id\"][\"value\"].rsplit(\"/\", 1)[-1]\n",
    "        label = row[\"label\"][\"value\"]\n",
    "        labels[qid] = label\n",
    "    time.sleep(PAUSE)\n",
    "\n",
    "print(f\"âœ” labels fetched ({len(labels):,})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40b983c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” graph built â†’ 214 nodes | 259 edges\n",
      "âœ” pickle saved â†’ math_graph.pkl\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€ StageÂ 4 â€“ build graph & write artifacts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "G = nx.DiGraph()\n",
    "for q in all_nodes:\n",
    "    G.add_node(q, label=labels.get(q, q))\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "print(f\"âœ” graph built â†’ {G.number_of_nodes():,} nodes | \"\n",
    "      f\"{G.number_of_edges():,} edges\")\n",
    "\n",
    "PICKLE = \"math_graph.pkl\"\n",
    "with open(PICKLE, \"wb\") as f:\n",
    "    pickle.dump(G, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"âœ” pickle saved â†’ {PICKLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aed4bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€£ generating interactive HTML â€¦ math_graph.html\n",
      "done.  Open math_graph.html in a browser.\n",
      "\n",
      "ðŸ All set â€“ enjoy exploring the mathematics universe!\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€ StageÂ 5 â€“ interactive PyVis graph â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"â€£ generating interactive HTML â€¦ \", end=\"\", flush=True)\n",
    "from pyvis.network import Network\n",
    "net = Network(height=\"900px\", width=\"100%\", directed=True,\n",
    "              bgcolor=\"#ffffff\", notebook=False)\n",
    "net.from_nx(G)\n",
    "net.toggle_physics(True)\n",
    "HTML = \"math_graph.html\"\n",
    "net.show(HTML, notebook=False)\n",
    "print(f\"done.  Open {HTML} in a browser.\\n\")\n",
    "\n",
    "print(\"ðŸ All set â€“ enjoy exploring the mathematics universe!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
